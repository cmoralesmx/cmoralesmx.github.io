<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="https://cmoralesmx.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cmoralesmx.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2022-11-09T02:05:00+00:00</updated><id>https://cmoralesmx.github.io/feed.xml</id><title type="html">blank</title><subtitle>Personal website of Carlos A. Morales, certified AWS Cloud Solutions Architect and Developer </subtitle><entry><title type="html">Legion 5P with hybrid graphics, no backlight issues</title><link href="https://cmoralesmx.github.io/blog/2022/legion-5P-hybrid-graphics/" rel="alternate" type="text/html" title="Legion 5P with hybrid graphics, no backlight issues"/><published>2022-11-07T07:00:00+00:00</published><updated>2022-11-07T07:00:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2022/legion-5P-hybrid-graphics</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2022/legion-5P-hybrid-graphics/"><![CDATA[<p>Hereby, I report the Legion 5P (15ARH05H) is no longer having backlight control problems with hybrid graphics. This seem to be fixed since mid 2022 with <code class="language-plaintext highlighter-rouge">nvidia</code> drivers newer than version <code class="language-plaintext highlighter-rouge">510</code>. In my case, I stayed with version <code class="language-plaintext highlighter-rouge">495.46</code> for a while, and did the jump to <code class="language-plaintext highlighter-rouge">520.56</code> back in October.</p> <div class="row mt-2 mb-2"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-11-07-nvidia-smi-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-11-07-nvidia-smi-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-11-07-nvidia-smi-1400.webp"/> <img src="/assets/img/2022-11-07-nvidia-smi.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This change also required an update in the parameters passed to the kernel as follows,</p> <p>For the nvidia drivers <code class="language-plaintext highlighter-rouge">495.46</code>, I was passing:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>amdgpu.backlight=0 
</code></pre></div></div> <p>Now, for the <code class="language-plaintext highlighter-rouge">nvidia</code> drivers version <code class="language-plaintext highlighter-rouge">520.56</code>, I am passing:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>acpi_backlight=video
</code></pre></div></div> <p>This driver is working correctly with kernel versions <code class="language-plaintext highlighter-rouge">5.15-lts</code> and <code class="language-plaintext highlighter-rouge">6.0</code>.</p> <p>Also, this laptop seems to be unaffected by the changes reported by <a href="https://hansdegoede.livejournal.com/">Hans de Goede</a> for <code class="language-plaintext highlighter-rouge">6.1</code> and <code class="language-plaintext highlighter-rouge">6.2</code>.</p>]]></content><author><name></name></author><category term="linux"/><summary type="html"><![CDATA[No more backlight control problems, and ready for kernel v6.1!]]></summary></entry><entry><title type="html">The Cloud Resume Challenge, AWS</title><link href="https://cmoralesmx.github.io/blog/2022/the-cloud-resume-challenge-aws/" rel="alternate" type="text/html" title="The Cloud Resume Challenge, AWS"/><published>2022-10-17T13:01:00+00:00</published><updated>2022-10-17T13:01:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2022/the-cloud-resume-challenge-aws</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2022/the-cloud-resume-challenge-aws/"><![CDATA[<p>I stumbled upon the <a href="https://cloudresumechallenge.dev/">Cloud Resume Challenge</a> while preparing for my first <code class="language-plaintext highlighter-rouge">AWS</code> certifications. However, I decided to prioritize and sit the examinations first before delving into this task.</p> <p>Once I achieved both the Solutions Architect and the Developer certifications, it was time to work on this.</p> <h2 id="what-is-the-cloud-resume-challenge">What is the Cloud Resume Challenge?</h2> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-cloudresume-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-cloudresume-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-cloudresume-1400.webp"/> <img src="/assets/img/2022-10-17-cloudresume.png" class="camg" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> <p>It is an experience-building activity aimed at presenting your resume using cloud technologies. It seems simple, right? Well, not quite. You are required to complete the tasks following <strong>best practices</strong>, which makes for a more dense project. The challenge consists of <a href="https://cloudresumechallenge.dev/docs/the-challenge/aws/">sixteen steps</a> taking you through the learning journey of automating the full life cycle of a web application using cloud technologies. Of course, before reaching such automation, a manual process must already be in place.</p> <p>Thus, you first learn to manually deliver your application to the cloud, in this case, a static website. Then, you make it publicly available through a custom domain name. And the whole task is completed by setting up every service needed yourself. Cloud technologies have simplified and reduced the cost of implementing this type of comprehensive project compared to doing the same on your hardware. However, weak spots exist in this version of the process due to the high human involvement required.</p> <p>Fortunately, you can reduce those weaknesses by automating the manual process. So, you learn to do such automation. By doing that, you also gain experience in the <strong>best practices</strong> prevailing among cloud leaders. These allow them to quickly react to changes in user demand, to deliver software more often and with higher quality, and to anticipate and recover from failure faster. Thus, you have the chance to gain a solid set of skills.</p> <p>If done correctly, the challenge takes you from zero to fluent in the cloud ecosystem of your choice within a few weeks.</p> <h2 id="what-was-my-motivation-for-this-challenge">What was my motivation for this challenge?</h2> <p>Not long ago, I stumbled upon a blog post describing how <code class="language-plaintext highlighter-rouge">Amazon</code> evolved their practices and tools until they successfully coped with the unprecedented surge in demand they observed on their first Black Friday sales.</p> <p>The post in question was not a heavily-technical one providing a high-level overview of <code class="language-plaintext highlighter-rouge">AWS</code> and how their publicly available platform and knowledge base can power others to achieve similar features. Yet, this post solved many of the questions I had from my time in the industry.</p> <p>For better or worst, my professional path revolved around early-stage start-ups (one of which was my own), highly interested in delivering software quickly but still adamant about adopting modern development practices. Thus, I experienced developing and deploying applications following mostly manual methods, and similarly, when executing operational activities, all done with no automation. Usually, the manual execution led to failures and sometimes even delaying the projects.</p> <p>Thus, the post I mentioned gave me hope for a better way of doing software projects. That was exciting news! Therefore, I wanted to get involved as soon as possible. Yet, with no expertise or colleges working in this area, the chances of making this a reality looked grim.</p> <p>Fortunately, this changed when I learned about this challenge! It seemed like an opportunity to dive into this area and increase my familiarity with the tools and techniques. Thus, I decided to tackle this project and opted to do so in AWS and here you will read my experience doing so.</p> <h2 id="my-experience">My Experience</h2> <p>Tackling this project helped me realize the theoretical foundation gained preparing to sit the certifications was insufficient for this new task. But, I am always happy to learn new things and hone my skills.</p> <p>Nevertheless, I had to fulfil the prerequisites before getting involved with the challenge,</p> <h3 id="prerequisites">Prerequisites</h3> <p><em>The certification</em>, the challenge calls for the Cloud Practitioner certification, yet as I said earlier, I had already obtained the Solutions Architect and Developer certifications which I decided to use instead.</p> <p><em>The resume</em>, my resume needed to be a static website created only with internet-native technologies, such as <code class="language-plaintext highlighter-rouge">Hypertext Mark-up Language (HTML)</code> for the text and semantic content and <code class="language-plaintext highlighter-rouge">Cascading Style Sheets (CSS)</code> for the styling. Such a simple site may seem extremely basic, yet the technology stack needed for such a website is also greatly simplified. Given the complexity of the project I was about to tackle, this simplification was beneficial.</p> <p>To my advantage, I was already using a static site generator to produce a personal blog which hosted an academic version of my resume. Thus, I devised a strategy to reuse parts of that site instead of creating one from scratch. For this, I took the last version of my site and branched from there, isolating the relevant content for the resume while the parent branch preserved the rest. After a few tweaks on both versions, I had the single repository ready and set up to produce the two sites needed. Being honest, I was not totally pleased by having both sites using the same layout and graphic design, but this had to do.</p> <div class="row mt-5 justify-content-md-center"> <div class="col-6 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-cmca-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-cmca-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-cmca-1400.webp"/> <img src="/assets/img/2022-10-17-cmca.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-6 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-resume-cmca-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-resume-cmca-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-resume-cmca-1400.webp"/> <img src="/assets/img/2022-10-17-resume-cmca.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="row mt-3 mb-5"> <div class="col mt-md-0" style="text-align: center"> My former website, on the left, and the resume site, on the right. </div> </div> <p>I later added a second statically generated site acting as a landing page for my resume. I am providing further detail near the end of the post.</p> <p><em>Account setup</em>, I already had an <code class="language-plaintext highlighter-rouge">AWS</code> account, so I did not create a new one for the challenge. Yet, I <strong>must</strong> touch on the <strong>security considerations</strong> I followed.</p> <p>Firstly, I set up the <code class="language-plaintext highlighter-rouge">root account</code> with a secure password and <code class="language-plaintext highlighter-rouge">2FA</code>, two-factor authentication, which transforms the standard login into a two-step process and requires the account owner to approve each login attempt via a physical device.</p> <p>Secondly, I set up a <code class="language-plaintext highlighter-rouge">budget</code> with <code class="language-plaintext highlighter-rouge">billing and usage alerts</code>. This way, I would get prompt notification of unusual increases in usage.</p> <p>For the challenge, I created two pairs of <code class="language-plaintext highlighter-rouge">IAM users</code> and <code class="language-plaintext highlighter-rouge">roles</code> in this account, each for specific tasks. The first one for executing administrative duties on the <code class="language-plaintext highlighter-rouge">AWS console</code> without programmatic access. The second one for developer-related activities with programmatic access to a subset of services. I also set up the latter for usage in the <code class="language-plaintext highlighter-rouge">AWS CLI</code>, which I had available on my local system since before the challenge.</p> <p>Finally, I locked away the <code class="language-plaintext highlighter-rouge">root account</code>, which I did not use anymore during the project.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-aws-signin-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-aws-signin-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-aws-signin-1400.webp"/> <img src="/assets/img/2022-10-17-aws-signin.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Securing a new or current account this way is <strong>crucial</strong> to prevent misuse, overcharges, and headaches. Plus, this is a first step towards the <code class="language-plaintext highlighter-rouge">principle of least privilege</code> which aims at reducing security exposure by providing just the right credentials for the activities to execute next.</p> <blockquote> <p>Please note this post does not include any code, and I do not provide a detailed recap of how I solved the most challenging bits. I decided this to prevent diluting the experience for others taking the challenge. All my code is available in a private repository in my GitHub account. I can provide access upon request for legitimate purposes.</p> </blockquote> <h2 id="stage-1-manual-labour">Stage 1, Manual labour</h2> <p>The first six steps of the challenge took me through the manual part of the process.</p> <p>I started by uploading the resume site to an <code class="language-plaintext highlighter-rouge">Amazon Simple Storage Service (S3)</code> <em>bucket</em> with public website hosting enabled. Doing so makes the website available via an <code class="language-plaintext highlighter-rouge">S3</code> website <em>endpoint</em>. Yet, the requirement calls for a custom domain name. Custom domain names must be set up with a <code class="language-plaintext highlighter-rouge">DNS</code> server, raising a new difficulty.</p> <p>Routing traffic from a <code class="language-plaintext highlighter-rouge">DNS</code> server directly to an <code class="language-plaintext highlighter-rouge">S3</code> bucket is impossible. Thus, I set up a <code class="language-plaintext highlighter-rouge">CloudFront</code> distribution, a <code class="language-plaintext highlighter-rouge">Content Delivery Network</code> providing fast access to your content from geographic regions distant from the physical location of the hardware hosting the website and acting as an intermediary between <code class="language-plaintext highlighter-rouge">Route53</code> and <code class="language-plaintext highlighter-rouge">S3</code>.</p> <p>Having my site hosted in the North-East region of the United States, any access from a different AWS region would face longer load times. Thankfully, the <code class="language-plaintext highlighter-rouge">CloudFront</code> distribution mitigated that by replicating the content to many servers to many servers across America and Europe. Thus, the website will load faster for anyone within these regions.</p> <p>And, just like that, the website was now live!</p> <p>Nevertheless, communication with a website should occur over an encrypted channel. This type of encryption requires a <code class="language-plaintext highlighter-rouge">Transport Layer Security (TLS)</code> certificate. In my case, <code class="language-plaintext highlighter-rouge">AWS Certificate Manager</code> produced the certificate for the website, and I only needed to set up <code class="language-plaintext highlighter-rouge">CloudFront</code> to use the certificate.</p> <p>Completing these activities would have been enough for a website hosted on the <code class="language-plaintext highlighter-rouge">AWS</code> public cloud with encryption enabled. Yet, there was a lot more ground to cover in the challenge.</p> <p>Usually, cloud applications interact with other services to improve the user experience. I needed to add a visitor counter and keep track of this number, meaning I needed to involve a programming language and a database. However, static site servers do not provide database access.</p> <p>I learnt about <code class="language-plaintext highlighter-rouge">DynamoDB</code>, a fully-featured <code class="language-plaintext highlighter-rouge">NoSQL</code> database for the cloud. This type of database differs from the formerly most common <code class="language-plaintext highlighter-rouge">SQL</code> or <code class="language-plaintext highlighter-rouge">relational</code> databases in several aspects, the most relevant of which is the way they store data and how they represent relationships.</p> <p>Regardless, accessing data from a database directly from the front end is not a good practice.</p> <p>I needed to implement a database-enabled back-end service, so the website could interact with it to store and retrieve data. Usually, this means setting up a database server and configuring your application to access the resource. From previous experience, I was aware these activities were not trivial.</p> <p>Luckily, <strong>serverless technologies</strong> abstract the details of building and running applications in the cloud. Thus, you can focus on implementing an application without worrying too much about the server side. Plus, in some cases, such as <code class="language-plaintext highlighter-rouge">AWS Lambda Functions</code>, they allow the application to be as minimal as a single function, which can have many benefits.</p> <p>But <code class="language-plaintext highlighter-rouge">AWS Lambda Functions</code> cannot operate autonomously to handle the current scenario. They need a directory and routing service to receive and process the external requests and invokes the correct function based on the parameters it receives. <code class="language-plaintext highlighter-rouge">API Gateway</code> is the service that does this in <code class="language-plaintext highlighter-rouge">AWS</code>.</p> <p>For implementing the functions enabling this interaction with <code class="language-plaintext highlighter-rouge">DynamoDB</code>, I took advantage of <code class="language-plaintext highlighter-rouge">boto3</code>, a <code class="language-plaintext highlighter-rouge">Python</code> framework from <code class="language-plaintext highlighter-rouge">AWS</code> exposing their services for easy consumption and integration. Doing so allowed me to easily and quickly complete the back-end application to enable the website to read and write data to and from the database.</p> <p>However, following best practices, I must have thoroughly tested the <code class="language-plaintext highlighter-rouge">Lambda functions</code> before release. Thus, I added unit test to make sure the code was working correctly. This testing should be executed routinely as the code base changes to check the functionality covered still works as expected. At this point in the challenge, I was manually executing these tests.</p> <p>After that, I set up <code class="language-plaintext highlighter-rouge">API Gateway</code> to invoke the <code class="language-plaintext highlighter-rouge">Lambda functions</code> in response to the requests coming from my website.</p> <p>And that was all the work needed for the first part. It was a long and winding process, but I had a working site hosted on the <code class="language-plaintext highlighter-rouge">AWS</code> cloud accessible via a custom domain name. Plus, I had certainty the code was working, and future changes that may break the functionality would not get released <strong>if and only if</strong> I were to run the tests before updating the public website.</p> <p>The infrastructure of my solution follows, providing a high-level overview of what services are involved and how they relate to each other.</p> <p> <br/></p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-the-crc-stack-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-the-crc-stack-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-the-crc-stack-1400.webp"/> <img src="/assets/img/2022-10-17-the-crc-stack.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> The infrastructure for my solution to the challenge </div> </div> <p> <br/></p> <p>The key takeaways for me from this first stage were,</p> <ol> <li>Securing and locking the <code class="language-plaintext highlighter-rouge">root account</code> is a <strong>MUST</strong>.</li> <li><code class="language-plaintext highlighter-rouge">IAM</code> users for the win!</li> <li><strong>NEVER</strong> embed credentials or secrets in code.</li> <li><code class="language-plaintext highlighter-rouge">Origin-Access Identity</code> is a more secure way of hosting a website on <code class="language-plaintext highlighter-rouge">S3</code>.</li> <li>A standard <code class="language-plaintext highlighter-rouge">DNS</code> server cannot choose the best server to use per request based on specific rules, but other services, such as <code class="language-plaintext highlighter-rouge">CloudFront</code> can.</li> <li>A <code class="language-plaintext highlighter-rouge">CloudFront</code> distribution is a valid target for traffic from a <code class="language-plaintext highlighter-rouge">DNS</code> server, and it can take an <code class="language-plaintext highlighter-rouge">S3</code> bucket as its origin.</li> <li>Requests from <code class="language-plaintext highlighter-rouge">CloudFront</code> to <code class="language-plaintext highlighter-rouge">S3</code> are <code class="language-plaintext highlighter-rouge">HTTP</code>, not <code class="language-plaintext highlighter-rouge">HTTPS</code>. <code class="language-plaintext highlighter-rouge">Amazon S3</code> does not expose an <code class="language-plaintext highlighter-rouge">HTTPS</code> <em>endpoint</em>.</li> <li>In some scenarios, <code class="language-plaintext highlighter-rouge">Lambda</code> functions may need <code class="language-plaintext highlighter-rouge">CORS</code> headers.</li> <li>A <code class="language-plaintext highlighter-rouge">CloudFront</code> distribution allows specifying domain name aliases to enable access to the cached content via custom domain names.</li> <li>Always mind the delay between creating or updating a <code class="language-plaintext highlighter-rouge">CloudFront</code> distribution and the files being ready at the edge locations.</li> </ol> <p>Configuring all the services is a time-consuming activity, plus with so many details to set up, the surface of exposure is considerable. Let’s improve this.</p> <p> <br/></p> <h2 id="stage-2-automation">Stage 2, Automation</h2> <p>To start this second part, I learnt to avoid manually configuring the <code class="language-plaintext highlighter-rouge">API</code> resources by leveraging the <code class="language-plaintext highlighter-rouge">AWS Serverless Application Model (SAM)</code> framework.</p> <p><code class="language-plaintext highlighter-rouge">AWS SAM</code> uses text-based templates to declare all the resources comprising a serverless application. These templates are part of a technology enabling the creation and provisioning of <code class="language-plaintext highlighter-rouge">AWS</code> infrastructure deployments predictably and repeatedly, also known as <code class="language-plaintext highlighter-rouge">Infrastructure as Code (IaC)</code>. <code class="language-plaintext highlighter-rouge">IaC</code> intends to treat infrastructure the same way developers treat code. Among others, <code class="language-plaintext highlighter-rouge">IaC</code> enables versioning of the source template to keep track of the changes to the infrastructure as the projects evolve. Thus, from this point, I started using the <code class="language-plaintext highlighter-rouge">Git</code> system for this project and <code class="language-plaintext highlighter-rouge">GitHub</code> to host the remote code repository.</p> <p>Now, that I was doing <code class="language-plaintext highlighter-rouge">IaC</code>, I was also executing part of the practices known as <code class="language-plaintext highlighter-rouge">DevOps</code>, short for development and operations. Other of these practices involve automating the integration and delivery pipeline. There are many tools available for these activities, such as <code class="language-plaintext highlighter-rouge">AWS CodePipeline</code>, or the more manual trio made of <code class="language-plaintext highlighter-rouge">AWS CodeCommit</code>, <code class="language-plaintext highlighter-rouge">AWS CodeBuild</code>, and <code class="language-plaintext highlighter-rouge">AWS CodeDeploy</code>, as well as non-AWS tools. I covered this part of the challenge using <code class="language-plaintext highlighter-rouge">GitHub Actions</code>. These are scripts running on <code class="language-plaintext highlighter-rouge">GitHub</code> servers and executing sequential tasks in response to code submissions. The <code class="language-plaintext highlighter-rouge">GitHub Actions</code> were responsible for running the unit tests and requiring all tests to pass before proceeding with the delivery of the <code class="language-plaintext highlighter-rouge">AWS SAM</code> template. Thus, this step added some certainty to my project in the remote case I submitted breaking changes.</p> <p>However, the <code class="language-plaintext highlighter-rouge">GitHub Actions</code> needed programmatic access to the <code class="language-plaintext highlighter-rouge">AWS</code> resources in my account to complete their execution. I enabled this by creating a set of credentials in <code class="language-plaintext highlighter-rouge">AWS IAM</code> with <strong>restricted authorization</strong> for the services required. Provisioning credentials requires careful consideration due to the inherent <strong>security implications</strong> involved. It may be easy to hardcode the credentials directly in the code. However, this is a highly unsafe decision even when working with private repositories and goes against best practices. The safer option, and my course of action, is to use <code class="language-plaintext highlighter-rouge">GitHub Actions secrets</code> to store this sensitive data in encrypted storage and request the values from within the <code class="language-plaintext highlighter-rouge">GitHub Actions</code>.</p> <p>For the next step, I focused my efforts on the front end. Thus, I created a second <code class="language-plaintext highlighter-rouge">GitHub</code> repository for the website and a new set of <code class="language-plaintext highlighter-rouge">GitHub Actions</code>. These <code class="language-plaintext highlighter-rouge">GitHub Actions</code> would synchronize the website files in <code class="language-plaintext highlighter-rouge">Amazon S3</code> whenever a new commit changed the statically generated site in <code class="language-plaintext highlighter-rouge">GitHub</code>. In this case, I added a second task to <em>invalidate</em> the files in the <code class="language-plaintext highlighter-rouge">CloudFront</code> distribution cache after each update forcing an update from the new files in <code class="language-plaintext highlighter-rouge">S3</code>. Not doing so could have undesired consequences for my website.</p> <p>At this point, I had almost completed the challenge. Yet, I was discontent with having my resume exposed directly without a landing page. So, I created a new version of the former landing page of my original static website and added another <code class="language-plaintext highlighter-rouge">GitHub Action</code> to the corresponding repository for publishing this landing page too.</p> <p>Finally, I had the resume published on a subdomain and the landing page on the top-level domain.</p> <blockquote> <p>This last step allowed me to experience more tools and techniques for hosting the website, distributing the content, monitoring, and cost optimizations beyond what the challenge required. More details are in the last section of the post.</p> </blockquote> <p>And, just like that, I was done with the challenge, well, almost. The final activity was to write a post about this experience. Thus, this post was born.</p> <p>The key takeaways for me from this second stage were,</p> <ol> <li><code class="language-plaintext highlighter-rouge">AWS SAM</code> is a subset of <code class="language-plaintext highlighter-rouge">CloudFormation</code> and may be easier for some scenarios.</li> <li><code class="language-plaintext highlighter-rouge">AWS SAM</code> and <code class="language-plaintext highlighter-rouge">CloudFormation</code> templates can directly deliver code to a target.</li> <li>It is possible to embed code in <code class="language-plaintext highlighter-rouge">SAM</code> and <code class="language-plaintext highlighter-rouge">CloudFormation</code> templates, but it is best to keep it short.</li> <li>Targeting remote services for <code class="language-plaintext highlighter-rouge">Unit Testing</code> can quickly elevate costs.</li> <li><strong>Mocking</strong> services for testing prevents unnecessary delays and charges, among other things.</li> <li>I must aim for modular template designs that enable reuse.</li> <li>It is best to use <code class="language-plaintext highlighter-rouge">CloudFormation</code> template parameters instead of hard-coded values.</li> <li>Stringent rules in the <code class="language-plaintext highlighter-rouge">CI/CD</code> pipeline, such as enforced style checks, can help maintain code standards.</li> <li>Failing to specify an alternate <code class="language-plaintext highlighter-rouge">CNAME</code> in a <code class="language-plaintext highlighter-rouge">CloudFormation</code> distribution leads to <code class="language-plaintext highlighter-rouge">403 errors</code>.</li> <li>Newly created certificates involve a manual validation step unless you automate the process.</li> <li>Having <code class="language-plaintext highlighter-rouge">CloudFormation</code> generate new certificates without specifying a timeout for the template execution may lead to a never-ending process due to pt.10.</li> <li>Taking care of the template runs is also taking care of the operational cost!.</li> </ol> <p>The challenge is designed to provide sufficient experience to hit the ground running as a junior cloud engineer. However, given the wide spectrum of what is possible with cloud technologies, it is also possible to take the project as a starting point and execute further exploration according to your own interests.</p> <h2 id="going-beyond-the-requirements-of-the-challenge">Going beyond the requirements of the challenge</h2> <p>Given my interests and time constraints, I explored a couple of optimization strategies and a more secure way for hosting the static website. A brief of these follows,</p> <h3 id="a-more-secure-way-of-hosting-a-static-website-in-s3">A more secure way of hosting a static website in <code class="language-plaintext highlighter-rouge">S3</code></h3> <p>I created an additional statically generated site to serve as the landing page for my resume. I also hosted this as a static site in <code class="language-plaintext highlighter-rouge">S3</code>, yet, instead of enabling <code class="language-plaintext highlighter-rouge">Static website hosting</code>, I set up an <code class="language-plaintext highlighter-rouge">Origin-Access Identity</code> in <code class="language-plaintext highlighter-rouge">CloudFront</code>. This feature enabled <code class="language-plaintext highlighter-rouge">CloudFront</code> to send authenticated requests to <code class="language-plaintext highlighter-rouge">Amazon S3</code> and set <code class="language-plaintext highlighter-rouge">S3</code> to allow access to authenticated requests from <code class="language-plaintext highlighter-rouge">CloudFront</code> only. This way, no <code class="language-plaintext highlighter-rouge">S3 website endpoint</code> is created either. Besides, this change also allowed me to explore using subdomains in <code class="language-plaintext highlighter-rouge">Route53</code> to expose the resume site while preserving the top-level domain for the landing page.</p> <h3 id="using-subdomain-specific-tls-certificates-in-cloudfront">Using subdomain-specific <code class="language-plaintext highlighter-rouge">TLS</code> certificates in <code class="language-plaintext highlighter-rouge">CloudFront</code></h3> <p><code class="language-plaintext highlighter-rouge">TLS</code> certificates encode information about the domain name they are validating, and they cannot change once they are issued. You must create a new certificate whenever changes are needed, such as updating some of its information. Thus, I needed to identify the simplest and most cost-effective option for updating the certificate that would better fit my scenario. For this, I also needed to consider each new certificate involves a manual authorization step in Route53.</p> <p>I decided to use two certificates instead of only one. I linked the first certificate to the top-level domain and the second to the subdomains. This option was also a good fit for the independent CI/CD pipelines producing the static sites and the infrastructure of my solution.</p> <div class="row mt-3 mb-5" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-tls-certs-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-tls-certs-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-tls-certs-1400.webp"/> <img src="/assets/img/2022-10-17-tls-certs.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> Information about both certificates side by side </div> </div> <h3 id="optimizing-operational-costs">Optimizing operational costs</h3> <p>As the project evolved, I continuously did access and cost assessments to identify opportunities for optimization. Doing so was key after one of the last updates, where I introduced new templates for both static sites. The new templates increased the footprint of the static sites quite sharply, which triggered the alerts I set up.</p> <p>After assessing the situation and identifying the culprits, I implemented a cost-reduction strategy via offloading resources to third-party CDNs and asset optimization.</p> <p>Thus, achieving a reduction of the operational cost by 50%</p> <div class="row mt-3 mb-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-cost-1-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-cost-1-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-cost-1-1400.webp"/> <img src="/assets/img/2022-10-17-cost-1.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> The landing page website file size before and after optimizations </div> </div> <div class="row mt-3 mb-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-cost-2-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-cost-2-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-cost-2-1400.webp"/> <img src="/assets/img/2022-10-17-cost-2.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> The resume website file size before and after optimizations </div> </div> <h2 id="closing-words">Closing words</h2> <p>Implementing the manual process was good as a learning exercise but distant from the best practices for cloud solutions. Applying <code class="language-plaintext highlighter-rouge">IaC</code> is not extremely difficult, and the gains far outweigh not doing so.</p> <p>Automated infrastructure provisioning is the way of the cloud.</p> <div class="row mt-3" style="text-align: center"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-10-17-aws-banner-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-10-17-aws-banner-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-10-17-aws-banner-1400.webp"/> <img src="/assets/img/2022-10-17-aws-banner.png" class="img-fluid rounded" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div>]]></content><author><name></name></author><category term="cloud,"/><category term="aws"/><summary type="html"><![CDATA[My experience completing the Cloud Resume Challenge in AWS]]></summary></entry><entry><title type="html">Cannot SSH to EC2 from Windows</title><link href="https://cmoralesmx.github.io/blog/2022/cannot-ssh-to-ec2-from-windows/" rel="alternate" type="text/html" title="Cannot SSH to EC2 from Windows"/><published>2022-01-12T08:00:00+00:00</published><updated>2022-01-12T08:00:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2022/cannot-ssh-to-ec2-from-windows</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2022/cannot-ssh-to-ec2-from-windows/"><![CDATA[<p>Usually, I am contacted to help resolve issues connecting to <code class="language-plaintext highlighter-rouge">EC2</code> instances from <code class="language-plaintext highlighter-rouge">Windows</code> computers. Two of the most common causes for these problems are,</p> <ul> <li>The use of an incorrect <code class="language-plaintext highlighter-rouge">username</code> for the target instance</li> <li>The credentials file has the wrong permissions</li> </ul> <p>For the first point, it is important to remember the <code class="language-plaintext highlighter-rouge">username</code> is related to the <code class="language-plaintext highlighter-rouge">AMI image</code> used to create the EC2 instance. Only the <code class="language-plaintext highlighter-rouge">Amazon AMI images</code> do have the username <code class="language-plaintext highlighter-rouge">ec2-user</code>, whereas <code class="language-plaintext highlighter-rouge">Ubuntu</code> images have the user <code class="language-plaintext highlighter-rouge">ubuntu</code>. Here’s a list of the default usernames in <code class="language-plaintext highlighter-rouge">AWS</code> <a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/TroubleshootingInstancesConnecting.html">source</a></p> <ul> <li>For <code class="language-plaintext highlighter-rouge">Amazon Linux 2</code> or the <code class="language-plaintext highlighter-rouge">Amazon Linux AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">ec2-user</code>.</li> <li>For a <code class="language-plaintext highlighter-rouge">CentOS AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">centos</code> or <code class="language-plaintext highlighter-rouge">ec2-user</code>.</li> <li>For a <code class="language-plaintext highlighter-rouge">Debian AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">admin</code>.</li> <li>For a <code class="language-plaintext highlighter-rouge">Fedora AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">fedora</code> or <code class="language-plaintext highlighter-rouge">ec2-user</code>.</li> <li>For a <code class="language-plaintext highlighter-rouge">RHEL AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">ec2-user</code> or <code class="language-plaintext highlighter-rouge">root</code>.</li> <li>For a <code class="language-plaintext highlighter-rouge">SUSE AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">ec2-user</code> or <code class="language-plaintext highlighter-rouge">root</code>.</li> <li>For an <code class="language-plaintext highlighter-rouge">Ubuntu AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">ubuntu</code>.</li> <li>For an <code class="language-plaintext highlighter-rouge">Oracle AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">ec2-user</code>.</li> <li>For a <code class="language-plaintext highlighter-rouge">Bitnami AMI</code>, the user name is <code class="language-plaintext highlighter-rouge">bitnami</code>.</li> </ul> <p>For the second point, evidenced by the message <code class="language-plaintext highlighter-rouge">Permissions for X are too open</code>, as seen in the next image.</p> <div class="row mt-2 mb-2"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-01-12-error-message-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-01-12-error-message-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-01-12-error-message-1400.webp"/> <img src="/assets/img/2022-01-12-error-message.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We need to make sure the credentials file’s permissions allow full control only to the desired windows user. This is done in Windows from the <code class="language-plaintext highlighter-rouge">Windows Explorer</code> as follows,</p> <p>1. Locate the credential file and <code class="language-plaintext highlighter-rouge">right-click</code> on it<br/> 2. From the pop-up menu, select <code class="language-plaintext highlighter-rouge">Properties</code>, and then, in the <code class="language-plaintext highlighter-rouge">Properties</code> <br/> dialogue box, click the <code class="language-plaintext highlighter-rouge">Security</code> tab. Then, click on “Advanced”</p> <div class="row mt-2 mb-2"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-01-12-file-properties-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-01-12-file-properties-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-01-12-file-properties-1400.webp"/> <img src="/assets/img/2022-01-12-file-properties.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>3. Set the desired user as the <code class="language-plaintext highlighter-rouge">Owner</code> of the file and disable inheritance.<br/> 4. Then, remove all entries in the <code class="language-plaintext highlighter-rouge">Permission entries</code> except the desired windows user.<br/> 5. The desired windows user should be allowed <code class="language-plaintext highlighter-rouge">Full control</code>.</p> <div class="row mt-2 mb-2"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-01-12-file-permissions-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-01-12-file-permissions-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-01-12-file-permissions-1400.webp"/> <img src="/assets/img/2022-01-12-file-permissions.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>6. The connection should now be successful</p> <div class="row mt-2 mb-2"> <div class="col-sm"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/2022-01-12-corrected-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/2022-01-12-corrected-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/2022-01-12-corrected-1400.webp"/> <img src="/assets/img/2022-01-12-corrected.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>I have found that these two cases solved many of the problems for my clients. Nevertheless, please get in contact If you are still having difficulties connecting to your <code class="language-plaintext highlighter-rouge">EC2</code> instances or if you need support. I can help you solve any issue you may be facing with your <code class="language-plaintext highlighter-rouge">AWS</code> resources.</p>]]></content><author><name></name></author><category term="cloud,"/><category term="aws"/><summary type="html"><![CDATA[Fix common problems trying to SSH to EC2 instances from Windows]]></summary></entry><entry><title type="html">Copying and Pasting between VIM instances</title><link href="https://cmoralesmx.github.io/blog/2020/copying-across-vim-instances/" rel="alternate" type="text/html" title="Copying and Pasting between VIM instances"/><published>2020-12-04T20:01:00+00:00</published><updated>2020-12-04T20:01:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2020/copying-across-vim-instances</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2020/copying-across-vim-instances/"><![CDATA[<p>One of the many features of the VIM editor is its multiple clipboards, “registers” in VIM lingo, for storing text (1 unnamed, 26 named, 10 numbered, 4 read-only, and 2 specials). All of them are accessed independently on-demand which is very useful for larger editing tasks.</p> <p>However, when the text must be copied to a file open in a different VIM instance, the situation is slightly different.</p> <div class="row mt-3 mb-3"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/2020-12-04-vim-nvim.png"/> </div> </div> <div class="caption"> Vim and NeoVim side by side </div> <p>Depending on how your VIM was built, copying and pasting text between instances may not be possible. This feature is usually not enabled on basic VIM installations. Checking if the feature is available can be done by executing,</p> <div class="language-vim highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">:</span>echo <span class="nb">has</span><span class="p">(</span><span class="s1">'clipboard'</span><span class="p">)</span>
</code></pre></div></div> <p>Which returns a boolean value.</p> <p>If the feature is not enabled, we could compile a custom VIM or use an alternative version such as gvim or even neoVIM. Neovim was born as a fork of VIM and is designed to be a drop-in replacement for the former and has this feature enabled by default.</p> <p>Both gvim and neovim can be installed with a single instruction,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo pacman -S gvim neovim
</code></pre></div></div> <p>Then, copying text between different instances of the same application can be achieved by taking advantage of two special registers,</p> <ul> <li>* for PRIMARY, which is the copy-on-select register and can be pasted using the mouse.</li> <li>+ for CLIPBOARD, this is accessed with the usual ^C and ^V</li> <li>+ for CLIPBOARD, this is accessed with the usual <kbd>Ctrl</kbd><kbd>Shft</kbd><kbd>C</kbd> and <kbd>Ctrl</kbd><kbd>Shft</kbd><kbd>V</kbd></li> </ul> <p>These registers are accessed similarly to the rest of the registers. For copying text, yanking in VIM lingo, to the registers just mentioned, you would do,<br/> <code class="language-plaintext highlighter-rouge">"*y</code> for the primary register<br/> <code class="language-plaintext highlighter-rouge">"+y</code> for the clipboard register<br/> <kbd>"</kbd><kbd>*</kbd><kbd>y</kbd> for the primary register<br/> <kbd>"</kbd> + <kbd>+</kbd> + <kbd>y</kbd> for the clipboard register</p> <p>For pasting the text, again, following the usual formula,<br/> <code class="language-plaintext highlighter-rouge">"+p</code> would paste from the primary register<br/> <code class="language-plaintext highlighter-rouge">"+p</code> would paste from the clipboard register</p> <p>Using these registers can be simplified by creating some key-binding such as,</p> <div class="language-vim highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">noremap</span> <span class="p">&lt;</span>Leader<span class="p">&gt;</span><span class="k">y</span> "*<span class="k">y</span>
<span class="nb">noremap</span> <span class="p">&lt;</span>Leader<span class="p">&gt;</span><span class="k">p</span> "*<span class="k">p</span>
<span class="nb">noremap</span> <span class="p">&lt;</span>Leader<span class="p">&gt;</span>Y "<span class="p">+</span><span class="k">y</span>
<span class="nb">noremap</span> <span class="p">&lt;</span>Leader<span class="p">&gt;</span>P "<span class="p">+</span><span class="k">p</span>
</code></pre></div></div> <p>You do not know what the <Leader> key is? By default, it is the backslash key <kbd> \ </kbd>. Or you can check what it is set to with</Leader></p> <div class="language-vim highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">:</span>echo mapleader
</code></pre></div></div> <p>In a future post, I will provide more ideas of its use.</p> <p>And that is it, copying and pasting text between instances is trivial again.</p>]]></content><author><name></name></author><category term="vim"/><summary type="html"><![CDATA[How to copy and paste text from one VIM instance to another]]></summary></entry><entry><title type="html">Using git to track your dot files</title><link href="https://cmoralesmx.github.io/blog/2020/git-store-dot-files/" rel="alternate" type="text/html" title="Using git to track your dot files"/><published>2020-12-03T22:01:00+00:00</published><updated>2020-12-03T22:01:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2020/git-store-dot-files</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2020/git-store-dot-files/"><![CDATA[<div class="row mt-3 mb-3"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/2020-12-03-git.gif"/> </div> </div> <div class="caption"> Using git to track your dot files </div> <p>In GNU/Linux, user-specific application configuration is traditionally stored as plain text in dot files (so-called because their filename starts with a dot). Given their file format is plain text, it is common to track their status using a version control system such as Git.</p> <p>One approach to do so is known as “bare repository and alias”, which entails doing the following,</p> <ol> <li>Initialize an empty -or bare- repository.</li> <li>Create an alias for this repo to avoid polluting the standard git</li> <li>Prevent files we haven not added explicitly from showing</li> <li>Register the new alias on profile initialisation files to engage our setup on each run automatically</li> </ol> <p>To do so, we must execute the following,</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>git init <span class="nt">--bare</span> <span class="nv">$HOME</span>/git/dotfiles
<span class="gp">$</span><span class="w"> </span><span class="nb">alias </span><span class="nv">config</span><span class="o">=</span><span class="s1">'/usr/bin/git --git-dir=$HOME/git/dotfiles/ --work-tree=$HOME'</span>
<span class="gp">$</span><span class="w"> </span>config config <span class="nt">--local</span> status.showUntrackedFiles no
<span class="gp">$</span><span class="w"> </span><span class="nb">echo</span> <span class="s2">"alias config='/usr/bin/git --git-dir=</span><span class="nv">$HOME</span><span class="s2">/git/dotfiles/ --work-tree=</span><span class="nv">$HOME</span><span class="s2">'"</span> <span class="o">&gt;&gt;</span> <span class="nv">$HOME</span>/.bashrc
</code></pre></div></div> <p>After this setup, git is ready to start tracking changes to any file inside $HOME. However, instead of the usual git commands, we use the ones we just setup</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>config setup  <span class="c"># instead of git setup</span>
</code></pre></div></div> <p>Then, we can add our files now</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ config add .gitconfig
$ config commit -m "Add gitconfig"
$ config add .bashrc
$ config commit -m "Add bashrc"
$ config add .makepkg.conf
$ config commit -m "Add makepkg"
$ config push
</code></pre></div></div> <p>Of course, a remote must be set up before pushing the changes. We can do so as follows,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ config remote add origin --your-remote-here--
$ config add README.md
$ config push --set-upstream origin master 
</code></pre></div></div> <p>From now on, we can use this repo to easily track our dot files</p>]]></content><author><name></name></author><category term="linux,"/><category term="git"/><summary type="html"><![CDATA[How to use git for tracking changes to your configuration files]]></summary></entry><entry><title type="html">Installing Arch Linux in a Lenovo Legion 5P, adding a GUI</title><link href="https://cmoralesmx.github.io/blog/2020/arch-install-gui/" rel="alternate" type="text/html" title="Installing Arch Linux in a Lenovo Legion 5P, adding a GUI"/><published>2020-12-02T20:01:00+00:00</published><updated>2020-12-02T20:01:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2020/arch-install-gui</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2020/arch-install-gui/"><![CDATA[<div class="row mt-3 mb-3"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/2020-12-02-arch-gui.png"/> </div> </div> <div class="caption"> Arch Linux in a Lenovo Legion 5P </div> <p>Following from the previous post where I documented the installation of the base Arch system, this post covers the steps taken to install a Graphical User Interface (GUI) to interact with my laptop.</p> <p>Again this post is intended to act as a personal guide for the future. So minimal description of the steps is available. If you are installing Arch, it would be wise to follow the official Install Guide.</p> <p>Because this second part deals with the GUI and the packages I use, if you decide to replicate the following steps most likely the software installed will not fully cover your needs.</p> <p>For systems were a GUI is essential, we can choose to install a Window Manager or a full Desktop Environment. I prefer xmonad and xfce4 correspondingly. Both of these depend on Xorg as the display server.</p> <h2 id="xorg">Xorg</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo pacman -S xorg xorg-xinit base-devel git firefox inetutils parted gvfs
$ sudo pacman -S linux-headers nvidia-dkms nvidia-settings opencl-nvidia opencl-headers 
</code></pre></div></div> <h2 id="xfce4-xmonad-and-utilities">xfce4, xmonad, and utilities</h2> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo pacman -S xfce4 xfce4-goodies networkmanager dnsmasq bluez bluez-utils pavucontrol
$ sudo pacman -S xmonad xmonad-contrib dmenu picom nitrogen xterm xmobar neovim
$ sudo pacman -S network-manager-applet nm-connection-editor
$ sudo pacman -S networkmanager-vpnc networkmanager-openvpn
$ sudo systemctl enable NetworkManager.service
$ sudo systemctl start NetworkManager.service
$ sudo pacman -S blueman pulseaudio-blueman pulseaudio-bluetooth
$ sudo systemctl enable bluetooth.service
$ sudo systemctl start bluetooth.service
$ sudo pacman -S qbittorrent remmina r tk openblas poppler gpicview peek doxygen dejagnu
$ sudo pacman -R poppler-glib poppler-qt5 qpdfview
$ sudo pacman -S poppler
$ sudo pacman -S texlive-core texlive-fontsextra pandoc ruby
$ sudo pacman -S ttf-freefont ttf-liberation ttf-droid
</code></pre></div></div> <h2 id="packages-from-the-aur">Packages from the AUR</h2> <p>these are already built</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo pacman -U ~/aur/packages/gcc8-*.tar.zst 
$ sudo pacman -U ~/aur/packages/cuda-10.2-*.tar.zst
$ sudo pacman -U ~/aur/packages/visual-studio-code-bin-1.51.1-1-x86_64.pkg.tar.zst
</code></pre></div></div> <p><em>Note:</em> The following patch must be installed after installing a new kernel. In my case, the last step also installed a patched kernel against which this module was built. If for any reason the patched module and the kernel lost sync, the module must be compiled again for the new kernel</p> <h3 id="fix-for-the-trackpad">fix for the trackpad</h3> <p>I need to replace <code class="language-plaintext highlighter-rouge">pinctrl-amd.ko.xz</code> with a patched version. The original module is <code class="language-plaintext highlighter-rouge">/lib/modules/$(uname -r)/kernel/drivers/pinctrl/pinctrl-amd.ko.xz</code></p> <p>The following steps backup and replace that file</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo cp /lib/modules/$(uname -r)/kernel/drivers/pinctrl/pinctrl-amd.ko.xz /lib/modules/$(uname -r)/kernel/drivers/pinctrl/pinctrl-amd.ko.xz.ORIGINAL
$ sudo cp ~/git/linux59_patches/touchpad/opt2_interrups/standone_pinctrl-amd/pinctrl-amd.ko /lib/modules/$(uname -r)/kernel/drivers/pinctrl/pinctrl-amd.ko.xz
</code></pre></div></div> <p>The trackpad should work after a reboot.</p> <h3 id="auto-mounting-shared-storage">Auto mounting shared storage</h3> <p>The following code must be added to /etc/ntfs</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># /dev/nvme0n1p6
UUID=13CA580F431F8673 /mnt/shared_data ntfs umask=000 0 1
</code></pre></div></div> <h3 id="wine-and-lutris">wine and lutris</h3> <p>I must enable multilib first</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo vim /etc/pacman.conf
</code></pre></div></div> <p>Now, I can install the packages</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo pacman -S wine lutris lib32-vulkan-icd-loader lib32-libxrandr
</code></pre></div></div> <p>However, I must disable non-essential libraries adding library override for mscoree=d;mshtml=d</p> <p>That is it. After a reboot the system should be ready.</p>]]></content><author><name></name></author><category term="linux,"/><category term="arch"/><summary type="html"><![CDATA[The second part of installing Arch on my laptop, the Graphical User Interface]]></summary></entry><entry><title type="html">Building packages from the AUR</title><link href="https://cmoralesmx.github.io/blog/2020/building-from-aur/" rel="alternate" type="text/html" title="Building packages from the AUR"/><published>2020-12-01T20:01:00+00:00</published><updated>2020-12-01T20:01:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2020/building-from-aur</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2020/building-from-aur/"><![CDATA[<div class="row mt-3 mb-3"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/2020-12-01-aur.png"/> </div> </div> <p>One of the interesting aspects of Arch Linux is its user-supported resources. This post will focus on one of those resources, the Arch User Repository (AUR)</p> <blockquote><p>The Arch User Repository (AUR) is a community-driven repository for Arch users. It contains package descriptions (PKGBUILDs) that allow you to compile a package from source with makepkg and then install it via pacman. The AUR was created to organize and share new packages from the community and to help expedite popular packages' inclusion into the community repository</p></blockquote> <p>Therefore, we can think of it as a massive collection of software (in source form) not available in the official repositories.</p> <p>Some may feel building these packages is hard so they recommend the use of helpers for this task, i.e.: yay, pacman, and similar. But in many cases, the build process is mostly automated by the build scripts available here.</p> <p>In my opinion, part of the reason for doing this my way is because I have set up the build tools to,</p> <ul> <li>make use of the n-cores in my system to shorten the time needed building the packages</li> <li>use RAM instead of persistent storage for the build process</li> <li>optimize the applications for a specific processor architecture</li> <li>keep the produced packages in a specific location for later reuse</li> </ul> <p>Those settings are stored in <code class="language-plaintext highlighter-rouge">~/.makepkg.conf</code> and a good guide for setting them is available at <a href="https://wiki.archlinux.org/index.php/makepkg#Tips_and_tricks">this entry</a> of the Arch Wiki.</p> <p>The whole process itself is fairly simple, it entails</p> <ol> <li>fetching the build package description files from the AUR</li> <li>creating the package (or packages)</li> <li>using pacman to install the produced package (or packages)</li> </ol> <p>In the next example, five packages are fetched and one gpg key is retrieved to validate one of the packages. This step is sometimes needed for some packages if the signing key of the package maintainer changed after the build file was made available.</p> <p>To fetch the build scripts we just need to git clone what we need,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd ~/aur
$ git clone https://aur.archlinux.org/visual-studio-code-bin.git
$ git clone https://aur.archlinux.org/browsh.git
$ git clone https://aur.archlinux.org/ipscan.git
$ git clone https://aur.archlinux.org/gcc8.git
$ gpg --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys A328C3A2C3C45C06
$ git clone https://aur.archlinux.org/cuda-10.2.git
</code></pre></div></div> <p>Cloning the needed files should be fairly fast.</p> <p>Now, each package is built in the same way by issuing the following command from each of the directories created by the clone process</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ makepkg -S
</code></pre></div></div> <p>After the build is completed, we instruct pacman to install using the packages produced by using the <code class="language-plaintext highlighter-rouge">-U</code> flag and the full path of the package,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo pacman -U ~/aur/packages/gcc8-*.tar.zst 
$ sudo pacman -U ~/aur/packages/cuda-10.2-*.tar.zst
$ sudo pacman -U ~/aur/packages/visual-studio-code-bin-1.51.1-1-x86_64.pkg.tar.zst
</code></pre></div></div> <p>That is it, you should be ready to build your own AUR packages.</p>]]></content><author><name></name></author><category term="arch,"/><category term="linux"/><summary type="html"><![CDATA[A simple guide for build packages from the AUR without helpers]]></summary></entry><entry><title type="html">Installing Arch Linux on a Lenovo Legion 5P</title><link href="https://cmoralesmx.github.io/blog/2020/arch-install/" rel="alternate" type="text/html" title="Installing Arch Linux on a Lenovo Legion 5P"/><published>2020-11-30T20:01:00+00:00</published><updated>2020-11-30T20:01:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2020/arch-install</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2020/arch-install/"><![CDATA[<div class="row mt-3 mb-3"> <div class="col-sm"> <img class="img-fluid rounded z-depth-1" src="/assets/img/2020-11-30-arch.jpg"/> </div> </div> <div class="caption"> Arch Linux on Lenovo Legion 5P </div> <p>In earlier posts, I documented how I solved a series of issues due to the Linux support in this laptop. Eventually, I found the lack of the <code class="language-plaintext highlighter-rouge">nvidia-dkms</code> package was adding unnecessary work for me. And because using packages from a different distribution is not supported or recommended, I decided to migrate my system to Arch Linux.</p> <p>Before going any further prease bear in mind this post is heavily technical. It follows the official <a href="https://wiki.archlinux.org/index.php/installation_guide">installation guide</a>, which is a must read for installing Arch, but the purpose of this post is to act as a personal reference for the future.</p> <h2 id="partition-schema">Partition schema</h2> <p>I have used the following schema on 500Gb and 1Tb drives</p> <pre><code class="language-pre">device		size	format	mont point	system
nvme1n1p1 	  260 	fat 	/mnt/boot	/boot
nvme1n1p2	20480	btrfs	/mnt		/
nvme1n1p3	12288	btrfs	/mnt/var	/var
nvme1n1p4	13312	swap	swap		swap
nvme1n1p5	all	btrfs	/mnt/home	/home
</code></pre> <h2 id="initial-steps">Initial steps</h2> <p>Enable the UK keyboard layout</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># loadkeys uk
</code></pre></div></div> <p>Try to connect directly to Wi-Fi as follows,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># iwctl --passphrase myPassPhrase station wlan0 connect targetWiFiNetwork
</code></pre></div></div> <p>If this fails, either enable the Wi-Fi devices</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># rfkill unblock wifi
</code></pre></div></div> <p>or try with the interactive way</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># iwctl
# station wlan0 scan
# station wlan0 get-networks
# station wlan0 connect targetWifiNetwork
# exit
</code></pre></div></div> <p>Once a connection is established, sync the system clock</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># timedatectl set-ntp true
</code></pre></div></div> <p>Then, setup the drive layout</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># fdisk -l # partition the drives according to the partition scheme shown earlier
# mkfs.btrfs /dev/nvme1n1p2 -f # root
# mkfs.btrfs /dev/nvme1n1p3 -f # var
# mkswap /dev/nvme1n1p4
# swapon /dev/nvme1n1p4
# mount /dev/nvme1n1p2 /mnt
# mkdir /mnt/boot
# mkdir /mnt/var
# mkdir /mnt/home
# mount /dev/nvme1n1p1 /mnt/boot
# mount /dev/nvme1n1p3 /mnt/var
# mount /dev/nvme1n1p5 /mnt/home
# reflector # choose fast servers
</code></pre></div></div> <p>Proceed with the base installation</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># pacstrap /mnt base linux5.8 linux-firmware amd-ucode btrfs-progs dhclient \
dhcpcd dosfstools efibootmgr iwd lynx man-db man-pages nvme-cli openssh rsync \
sudo texinfo tmux usbutils gvim vpnc zsh xf86-video-amdgpu vulkan-radeon 
</code></pre></div></div> <h2 id="configure-the-system">Configure the system</h2> <p>Write the current file system structure to fstab</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># genfstab -U /mnt &gt;&gt; /mnt/etc/fstab
# cat /mnt/etc/fstab
</code></pre></div></div> <p>Set the time zone and adjust the system clock</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># arch-chroot /mnt
# ln -sf /usr/share/zoneinfo/Europe/London /etc/localtime # set the timezone
# hwclock --systohc # generate /etc/adjtime
</code></pre></div></div> <p>Edit <code class="language-plaintext highlighter-rouge">/etc/locale.gen</code> and uncomment the locales needed,</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>en_GB.UTF-8 UTF-8
en_US.UTF-8 UTF-8
</code></pre></div></div> <p>Generate the locales,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># locale-gen
</code></pre></div></div> <p>And set the system locale by editing <code class="language-plaintext highlighter-rouge">/etc/locale.conf</code> adding:</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LANG=en_GB.UTF-8
</code></pre></div></div> <p>Set the keyboard layout persistently</p> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code># vim /etc/vconsole.conf
KEYMAP=uk
</code></pre></div></div> <p>Set the host name</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># vim /etc/hostname
</code></pre></div></div> <p><em>Note:</em> The only content of this file should be the name for this machine</p> <p>Add default addresses to <code class="language-plaintext highlighter-rouge">hosts</code> file</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># vim /etc/hosts
</code></pre></div></div> <pre><code class="language-pre">127.0.0.1	localhost
::1		localhost
127.0.1.1	arch-usb.localdomain	arch-usb
</code></pre> <p>Generate the initial minimal file system <code class="language-plaintext highlighter-rouge">initramfs</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># mkinitcpio -P
</code></pre></div></div> <p>Set the root password</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># passwd
</code></pre></div></div> <p>Add a non root user</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># useradd -m cmoralesmx
# passwd cmoralesmx
# usermod -aG wheel,audio,video,optical,storage cmoralesmx
</code></pre></div></div> <p>Enable users in wheel group to execute anything as sudo</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># EDITOR=vim visudo
</code></pre></div></div> <p><em>Note:</em> Search and uncomment the line about wheel users</p> <p>Remove unnecessary packages from the install</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># pacman -R modemmanager, mobile-broadband-provider-info
</code></pre></div></div> <p>Install a bootloader <em>Note:</em> I was using grub but switched to systemd-boot which seems leaner.</p> <p>Because I am already using systemd-boot, the loader configuration and the entries should be available and up to date in <code class="language-plaintext highlighter-rouge">/boot</code></p> <p>Therefore, I just need to install the actual bootloader</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># bootctl install
</code></pre></div></div> <p>At this point the base system is ready. For a headless computer this is enough.</p> <p>Optionally, I could install a Window Manager or a full Desktop Environment. I will cover that in the <a href="/blog/2020/arch-install-gui.html">next post</a></p>]]></content><author><name></name></author><category term="linux,"/><category term="arch"/><summary type="html"><![CDATA[The steps I followed for installing Arch Linux on my laptop]]></summary></entry><entry><title type="html">Legion 5P, touchpad non responsive fix</title><link href="https://cmoralesmx.github.io/blog/2020/legion-5P-touchpad-nonresponsive-fix/" rel="alternate" type="text/html" title="Legion 5P, touchpad non responsive fix"/><published>2020-11-28T20:01:00+00:00</published><updated>2020-11-28T20:01:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2020/legion-5P-touchpad-nonresponsive-fix</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2020/legion-5P-touchpad-nonresponsive-fix/"><![CDATA[<p>The Legion 5P-15ARH05H has an MSFT0001:00 06CB:7F28 touchpad which was not responding to input. A fix is mentioned in this <a href="https://www.linux.org/threads/lenovo-legion-5-touchpad.29536/page-2">linux.org thread</a> It entails enabling pooling on the device. This is achievable by creating the following two files in <code class="language-plaintext highlighter-rouge">~/Documents</code>,</p> <p><code class="language-plaintext highlighter-rouge">fix_trackpad.service</code> with the following content,</p> <div class="language-systemd highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">[Unit]</span>
<span class="nt">Description</span><span class="p">=</span>Fix the trackpad on the Legion 5P
 
<span class="k">[Service]</span>
<span class="nt">ExecStart</span><span class="p">=</span>/usr/bin/fix_trackpad.sh
 
<span class="k">[Install]</span>
<span class="nt">WantedBy</span><span class="p">=</span>multi-user.target
</code></pre></div></div> <p>And <code class="language-plaintext highlighter-rouge">fix_trackpad.sh</code> with the following content,</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/sh</span>
<span class="nb">echo </span>386 <span class="o">&gt;</span> /sys/class/gpio/export
<span class="nb">echo </span>out <span class="o">&gt;</span> /sys/class/gpio/gpio386/direction
</code></pre></div></div> <p>Then, issuing the following instructions once to set up the fix to be applied after every reboot or cold start,</p> <div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span><span class="nb">sudo cp</span> ~/Documents/fix_trackpad.sh /usr/bin/
<span class="nv">$ </span><span class="nb">sudo cp</span> ~/Documents/fix_trackpad.service /etc/systemd/system/
<span class="nv">$ </span><span class="nb">sudo </span>systemctl <span class="nb">enable </span>fix_trackpad
</code></pre></div></div> <p>Yet, additional reading showed pooling is not the best solution for this.</p> <p>The recommended fix, discussed in <a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1887190/+index?comments=all">bug #1887190</a>, is based on interrupts instead of polling. Interrupts are better than polling for power management.<br/> How do we fix this? <a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1887190/comments/189">comment #189</a> contains code which is a stand-alone version of the solution in comment #171.<br/> This stand-alone version is only useful if the module <code class="language-plaintext highlighter-rouge">pinctrl-amd</code> is built out of the kernel. To check if that is the case, we must execute,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ modinfo pinctrl-amd
</code></pre></div></div> <p>For arch-based distributions this is the case so the stand-alone solution works. Based on these findings, we must compile and install the module.<br/> The instructions to do so are available on the same comment #189 linked earlier.</p>]]></content><author><name></name></author><category term="linux"/><summary type="html"><![CDATA[Steps to fix the touchpad non responsive issue in Lenovo Legion 5P]]></summary></entry><entry><title type="html">Legion 5P, HDMI-out with hybrid graphics</title><link href="https://cmoralesmx.github.io/blog/2020/legion-5P-hdmi-out-with-hybrid-graphics/" rel="alternate" type="text/html" title="Legion 5P, HDMI-out with hybrid graphics"/><published>2020-11-21T20:00:00+00:00</published><updated>2020-11-21T20:00:00+00:00</updated><id>https://cmoralesmx.github.io/blog/2020/legion-5P-hdmi-out-with-hybrid-graphics</id><content type="html" xml:base="https://cmoralesmx.github.io/blog/2020/legion-5P-hdmi-out-with-hybrid-graphics/"><![CDATA[<blockquote> <p>Updated on November 2022: HDMI-out with hybrid graphics is not an issue any more for this computer. Here’s my <a href="/blog/2022/legion-5P-hybrid-graphics">post</a> about it</p> </blockquote> <p>Firstly, I tested if this was fixed in a different kernel version and by changing the version of the drivers. I tried with kernel versions 5.4, to 5.10. Only version 5.6 appeared to have everything working correctly.</p> <p>Therefore, for a couple of days, I was using kernel v5.6 and everything worked, switchable graphics worked, the LCD brightness-control worked, and HDMI-out! Everything was good until it was not. Surprisingly, a system update retired v5.6 of my system. I totally forgot this version of the kernel had reached EOL earlier this year!</p> <p>So, I had no option but to move to a newer kernel and try to fix this issue. I opted for v5.9 and proceeded to check the journal after a restart.</p> <p>The system’s journal was recording the following error when the external monitor was connected,</p> <div class="l-page-outset"> <figure class="highlight"><pre><code class="language-systemd" data-lang="systemd"><span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">Unhandled</span> <span class="err">error</span> <span class="err">in</span> <span class="err">__nv_drm_gem_user_memory_handle_vma_fault:</span> <span class="err">-22</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">WARNING:</span> <span class="err">CPU:</span> <span class="err">13</span> <span class="err">PID:</span> <span class="err">1595</span> <span class="err">at</span> <span class="err">/storage/manjaro/makepkg/linux59-nvidia-&gt;</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">Modules</span> <span class="err">linked</span> <span class="err">in:</span> <span class="err">ccm</span> <span class="err">rfcomm</span> <span class="err">fuse</span> <span class="err">cmac</span> <span class="err">algif_hash</span> <span class="err">algif_skcipher</span> <span class="err">af_a&gt;</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">cec</span> <span class="err">rc_core</span> <span class="err">drm</span> <span class="err">agpgart</span> <span class="err">syscopyarea</span> <span class="err">sysfillrect</span> <span class="err">sysimgblt</span> <span class="err">fb_sys_fops&gt;</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">CPU:</span> <span class="err">13</span> <span class="err">PID:</span> <span class="err">1595</span> <span class="err">Comm:</span> <span class="err">Xorg</span> <span class="err">Tainted:</span> <span class="err">P</span>           <span class="err">OE</span>     <span class="err">5.9.8-2-MANJA&gt;</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">Hardware</span> <span class="err">name:</span> <span class="err">LENOVO</span> <span class="err">82GU/LNVNB161216,</span> <span class="err">BIOS</span> <span class="err">FSCN09WW</span> <span class="err">06/28/2020</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">RIP:</span> <span class="err">0010:__nv_drm_gem_user_memory_handle_vma_fault+0x8c/0x90</span> <span class="err">[nvidia_&gt;</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">Code:</span> <span class="err">41</span> <span class="err">bc</span> <span class="err">00</span> <span class="err">01</span> <span class="err">00</span> <span class="err">00</span> <span class="err">44</span> <span class="err">89</span> <span class="err">e0</span> <span class="err">41</span> <span class="err">5c</span> <span class="err">c3</span> <span class="err">0f</span> <span class="err">0b</span> <span class="err">89</span> <span class="err">c2</span> <span class="err">48</span> <span class="err">c7</span> <span class="err">c6</span> <span class="err">80</span> <span class="err">d6</span> <span class="err">0&gt;</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">RSP:</span> <span class="err">0018:ffffad7340d03b78</span> <span class="err">EFLAGS:</span> <span class="err">00010286</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">RAX:</span> <span class="err">0000000000000000</span> <span class="err">RBX:</span> <span class="err">ffffad7340d03bc8</span> <span class="err">RCX:</span> <span class="err">0000000000000000</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">RDX:</span> <span class="err">0000000000000001</span> <span class="err">RSI:</span> <span class="err">ffffffffb2b8941a</span> <span class="err">RDI:</span> <span class="err">00000000ffffffff</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">RBP:</span> <span class="err">ffff961954d6a578</span> <span class="err">R08:</span> <span class="err">000000000000051f</span> <span class="err">R09:</span> <span class="err">0000000000000001</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">R10:</span> <span class="err">0000000000000000</span> <span class="err">R11:</span> <span class="err">0000000000000001</span> <span class="err">R12:</span> <span class="err">0000000000000002</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">R13:</span> <span class="err">0000000000000000</span> <span class="err">R14:</span> <span class="err">ffff961954d6a578</span> <span class="err">R15:</span> <span class="err">ffffad7340d03bc8</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">FS:</span>  <span class="err">00007f30b43b6540(0000)</span> <span class="err">GS:ffff9619af740000(0000)</span> <span class="err">knlGS:0000000000&gt;</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">CS:</span>  <span class="err">0010</span> <span class="err">DS:</span> <span class="err">0000</span> <span class="err">ES:</span> <span class="err">0000</span> <span class="err">CR0:</span> <span class="err">0000000080050033</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">CR2:</span> <span class="err">0000557ce4b06db0</span> <span class="err">CR3:</span> <span class="err">00000007bb5ae000</span> <span class="err">CR4:</span> <span class="err">0000000000350ee0</span>
<span class="err">legion5P</span> <span class="err">kernel:</span> <span class="err">Call</span> <span class="err">Trace:</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">__do_fault+0x38/0xd0</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">handle_mm_fault+0x1496/0x1a40</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">__get_user_pages+0x25f/0x7c0</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">__gup_longterm_locked+0x61/0x1e0</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">os_lock_user_pages+0xa5/0x190</span> <span class="k">[nvidia]</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">_nv000635rm+0x7a/0xf0</span> <span class="k">[nvidia]</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">?</span> <span class="err">_nv000710rm+0x70c/0x880</span> <span class="k">[nvidia]</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">?</span> <span class="err">_raw_spin_unlock_irqrestore+0x20/0x40</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">?</span> <span class="err">rm_ioctl+0x54/0xb0</span> <span class="k">[nvidia]</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">?</span> <span class="err">nvidia_ioctl+0x5b7/0x900</span> <span class="k">[nvidia]</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">?</span> <span class="err">nvidia_frontend_unlocked_ioctl+0x37/0x50</span> <span class="k">[nvidia]</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">?</span> <span class="err">__x64_sys_ioctl+0x83/0xb0</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">?</span> <span class="err">do_syscall_64+0x33/0x40</span>
<span class="err">legion5P</span> <span class="err">kernel:</span>  <span class="err">?</span> <span class="err">entry_SYSCALL_64_after_hwframe+0x44/0xa9</span></code></pre></figure> </div> <p>I checked the Nvidia developer forum and the manjaro forum, luckly the <a href="https://forums.developer.nvidia.com/t/ryzen-7-gtx-1660ti-blank-screen-on-external-outputs-in-hybrid-graphics-mode/157800/4">solution</a> was already known and a <a href="https://patchwork.kernel.org/project/linux-arm-kernel/patch/20200513133245.6408-5-m.szyprowski@samsung.com/">patch</a> was available for the kernel. After checking if the patch was already applied to Manjaro or upstream latest versions, I could not find anything. So, it was clear I needed to patch and build my own kernel.</p> <p>I followed a combination of <a href="https://archived.forum.manjaro.org/t/how-to-compile-the-mainline-kernel-the-manjaro-way/51700/10">this guide</a> and <a href="https://wiki.archlinux.org/index.php/Patching_packages">Patching_packages</a>, the end result was as follows,</p> <ol> <li>I cloned the package repo for <a href="https://gitlab.manjaro.org/packages/core/linux59">v5.9</a> to <code class="language-plaintext highlighter-rouge">~/git/linux59</code></li> <li>added a new file with the patch needed</li> <li>added an entry to the <code class="language-plaintext highlighter-rouge">PKGBUILD</code> file to include the new file with the patch</li> <li>set <code class="language-plaintext highlighter-rouge">CONFIG_TRANSPARENT_HUGEPAGE=n</code> in <code class="language-plaintext highlighter-rouge">config</code> (inside the cloned directory)</li> <li>updated the sha512 sums array by executing: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ makepkg -g &gt;&gt; PKGBUILD
</code></pre></div> </div> </li> <li>build the kernel package. Here, I had two options, the manjaro way and the upstream way<br/> a. the Manjaro way: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd ~/git &amp;&amp; buildpkg -p linux59  
</code></pre></div> </div> <p>b. the upstream way:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd ~/git/linux59 &amp;&amp; makepkg -Csf
</code></pre></div> </div> </li> <li>archived the produced packages for future use</li> <li>installed the kernel and the headers <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo pacman -U linux59-5.9.8-2-x86_64.pkg.tar.zst linux59-headers-5.9.8-2-x86_64.pkg.tar.zst
</code></pre></div> </div> </li> <li>rebooted, selecting the new kernel</li> </ol> <p>The system booted but there was no video. I tried reinstalling the drivers but the result was the same. Further examination of the journal with,</p> <div class="language-console highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gp">$</span><span class="w"> </span>journalctl <span class="nt">-b0</span> <span class="nt">-p4</span>
</code></pre></div></div> <p>Showed that, in my first attempt, I failed to disable the option from <strong>Step 4</strong>. As a result, the fix was incomplete! Disabling the option still lead to the boot process ending on a blank screen but a different set of errors were recorded in the journal.</p> <p>Firstly, the error <code class="language-plaintext highlighter-rouge">AMD-Vi: Unable to read/write to IOMMU perf counter</code> should be fixed by adding a GRUB parameter (iommu=soft).<br/> Then, <code class="language-plaintext highlighter-rouge">Failed to get backlight or LED device 'backlight:acpi_video0': No such device</code>, should also be fixed by adding a GRUB parameter (acpi_backlight=vendor)</p> <p>To add those two parameters using GRUB I did,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo vim /etc/default/grub
</code></pre></div></div> <p>and appended <code class="language-plaintext highlighter-rouge">iommu=soft acpi_backlight=vendor</code> to the <code class="language-plaintext highlighter-rouge">GRUB_CMDLINE_LINUX_DEFAULT</code> variable.<br/> Then, updated grub by doing:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo grub-mkconfig -o /boot/grub/grub.cfg
</code></pre></div></div> <p>The following reboot revealed a new problem with <code class="language-plaintext highlighter-rouge">i2c-nvidia-gpu</code>, which was fixed by another kernel patch. The solution was found <a href="https://bugzilla.kernel.org/show_bug.cgi?id=206653">here</a>.</p> <p><em>Note</em>: At this point, my fork of the kernel with the corresponding patches is stored in my <a href="https://github.com/cmoralesmx/linux59/tree/hybrid_video">github repo</a>. This should simplify keeping up with upstream when future updates arrive and the patches need to be applied again.</p> <p>Then, thanks to the message <code class="language-plaintext highlighter-rouge">Nvidia: disagrees about version of symbol module_layout.</code>, I learned that the Nvidia drivers installed using the <code class="language-plaintext highlighter-rouge">mhwd</code> tool from Manjaro were not compatible with my custom kernel. Therefore, I needed to compile the required modules.</p> <p>I went back to the Manjaro repo and searched for the package <code class="language-plaintext highlighter-rouge">hybrid-amd-nvidia-455xx-prime</code> but there was no such package! How does Manjaro handle hybrid videon? I traced the installation of the drivers and found that <code class="language-plaintext highlighter-rouge">/var/lib/mhwd/db/pci/graphic_drivers/hybrid-amd-nvidia-455xx-prime/MHWDCONFIG</code> contains all the details needed. The actual packages needed are,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DEPENDS="nvidia-455xx-utils nvidia-prime"
DEPENDS_64="lib32-nvidia-455xx-utils"
DEPKMOD="nvidia-455xx"
</code></pre></div></div> <p>To build these packages, I followed this <a href="https://medium.com/@evintheair/building-a-custom-kernel-in-manjaro-linux-186da6a1cedf">short guide</a></p> <p>For building them, I cloned their build scripts for the individual packages from Manjaro’s repo and each was built as follows,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ buildpkg -p nvidia-455xx-utils
</code></pre></div></div> <p>producing: <code class="language-plaintext highlighter-rouge">nvidia-455xx-utils-455.45.01-1-x86_64.pkg.tar.zst</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ buildpkg -p lib32-nvidia-455xx-utils
</code></pre></div></div> <p>producing: <code class="language-plaintext highlighter-rouge">lib32-nvidia-455xx-utils-455.45.01-1-x86_64.pkg.tar.zst</code> and <code class="language-plaintext highlighter-rouge">lib32-opencl-nvidia-455xx-455.45.01-1-x86_64.pkg.tar.zst</code></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ buildpkg -p nvidia-455xx
</code></pre></div></div> <p>producing: <code class="language-plaintext highlighter-rouge">linux59-nvidia-455xx-455.45.01-2-x86_64.pkg.tar.zst</code></p> <p>Once built, the packages were moved to <code class="language-plaintext highlighter-rouge">/var/cache/manjaro-tools/pkg/stable/x86_64</code></p> <p>I thought because I was building the packages the Manjaro way, that location could be a default path prepared to enable installing using <code class="language-plaintext highlighter-rouge">mhwd</code> in the traditional way but trying to do so proved this was not the case. My packages were being ignored.</p> <p>It turns out my assumption was erroneous. The default location where the built packages were stored was not directly related to where pacman reads the local packages from which is <code class="language-plaintext highlighter-rouge">/var/cache/pacman/pkg/</code></p> <p>Therefore, after gathering the needed packages in a specific directory <code class="language-plaintext highlighter-rouge">~/git/custom_kernel_pkgs/</code>, I installed them directly by doing,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ sudo pacman -U ~/git/custom_kernel_pkgs/nvidia-455xx-utils-455.45.01-1-x86_64.pkg.tar.zst ~/git/custom_kernel_pkgs/linux59-nvidia-455xx-455.45.01-1-x86_64.pkg.tar.zst  ~/git/custom_kernel_pkgs/opencl-nvidia-455xx-455.45.01-1-x86_64.pkg.tar.zst
</code></pre></div></div> <p>Another reboot later still got me the same error,</p> <blockquote> <p><code class="language-plaintext highlighter-rouge">Nvidia: disagrees about version of symbol module_layout.</code></p> </blockquote> <p>What was going on?</p> <p>During the attempts building with <code class="language-plaintext highlighter-rouge">buildpkg</code>, I noticed some packages were downloaded before preparing the chroot environment but did not pay much attention to it.</p> <p>Several failed attempts later, I tried building the Nvidia packages again but using <code class="language-plaintext highlighter-rouge">makepkg</code>.</p> <p>First the <code class="language-plaintext highlighter-rouge">nvidia-utils</code>,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd ~/git/nvidia-455xx-utils/;
$ makepkg -sf
$ mv *.tar.zst ~/git/custom_pkgs/
$ sudo pacman -U ~/git/custom_pkgs/nvidia-455xx-utils-455.45.01-1-x86_64.pkg.tar.zst
</code></pre></div></div> <p>Then, the <code class="language-plaintext highlighter-rouge">linux59-nvidia-455xx</code>,</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>$ cd ~/git/nvidia-455xx/
$ makepkg -sf
$ mv *.tar.zst ~/git/custom_pkgs/
$ sudo pacman -U ~/git/custom_pkgs/linux59-nvidia-455xx-455.45.01-1-x86_64.pkg.tar.zst
</code></pre></div></div> <p>Another reboot and finally, after these attempts and many others non-documented here, this final attempt was successful!</p> <p>The HDMI-out was finally working in hybrid graphics mode and there were no kernel errors due to the Nvidia drivers. Success!</p> <p>The only thing missing is the display brightness.</p> <p>A side note: Based on my experience with this last part, I think by building the modules with <code class="language-plaintext highlighter-rouge">buildpkg</code>, the modules were compiled against a stock kernel every time. Whereas, using <code class="language-plaintext highlighter-rouge">makepkg</code> no chroot environment is recreated for each build or whatever kernel is in the system is referenced for the build so the packages are built against the correct kernel. I still need to check what the specific difference is between building with one or the other tool. I may cover that in another post.</p> <p>That is it for now. Congratulations if you made it to the end</p>]]></content><author><name></name></author><category term="linux"/><summary type="html"><![CDATA[Steps to fix the HDMI-out port using hybrid graphics in Lenovo Legion 5P]]></summary></entry></feed>